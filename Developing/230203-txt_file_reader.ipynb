{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782b7efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extracting portions of txt files ref : https://www.computerhope.com/issues/ch001721.htm\n",
    "## Config\n",
    "# input_txt = r'result_Be8-copy.txt'\n",
    "input_txt = 'result_Be8_HTDA_V0_800_FuLL.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "973dd6e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Package used\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "from fractions import Fraction\n",
    "import numpy as np\n",
    "\n",
    "### Functions used\n",
    "def remove_line(energy_list:\"list\",line_pattern:'str')->\"list\":\n",
    "    pattern_target = re.compile(line_pattern)\n",
    "    for line in energy_list:\n",
    "        if pattern_target.search(line) != None:\n",
    "            energy_list.remove(line)\n",
    "    return energy_list\n",
    "\n",
    "def remove_character(the_string:'str', to_remove_char:'str')->'str':\n",
    "    pattern_target = re.compile(to_remove_char)\n",
    "    to_replace = list(to_remove_char)\n",
    "    if pattern_target.search(the_string) != None:\n",
    "        for char in to_replace:\n",
    "            the_string = the_string.replace(char,\"\")\n",
    "    return the_string\n",
    "\n",
    "def identify_line(text_file:'str',pattern_start:'str', pattern_end:'str')->tuple:\n",
    "    pat_start = re.compile(pattern_start)\n",
    "    pat_end = re.compile(pattern_end)\n",
    "    line_start_counter = []\n",
    "    line_end_counter = []\n",
    "    \n",
    "    for counter, line in enumerate(text_whole):\n",
    "        if pat_start.search(line) != None:\n",
    "            line_start_counter.append(counter)\n",
    "        else:\n",
    "            pass\n",
    "        if pat_end.search(line):\n",
    "            line_end_counter.append(counter)\n",
    "        else:\n",
    "            pass\n",
    "    output_line_counter = line_start_counter + line_end_counter\n",
    "    return output_line_counter\n",
    "\n",
    "def degenerate_pair_gen(num_nucleon_orbitals:\"tuple\")->\"list\":\n",
    "    # num_nucleon_orbitals: (num_neut_orbitals, num_prot_orbitals)\n",
    "    pair = []; quad=[]\n",
    "    pair_list=[]; quad_list=[]\n",
    "    neut_orbitals_list = list(range(0,num_nucleon_orbitals[0]))\n",
    "    prot_orbitals_list = list(range(num_nucleon_orbitals[0], sum(num_nucleon_orbitals)))\n",
    "    matching_lvl = min(len(neut_orbitals_list),len(prot_orbitals_list))\n",
    "    for i in range(0,matching_lvl):\n",
    "        if len(quad) < 4:\n",
    "            quad.extend([neut_orbitals_list[i],prot_orbitals_list[i]])\n",
    "        elif len(quad)==4:\n",
    "            quad.sort(); quad_list.append((quad))\n",
    "            quad = []; quad.extend([neut_orbitals_list[i],prot_orbitals_list[i]])\n",
    "    if len(quad)==4:\n",
    "        quad.sort(); quad_list.append(list(quad))\n",
    "        quad = []; quad.extend([neut_orbitals_list[i],prot_orbitals_list[i]])\n",
    "    for quad_dum in quad_list:\n",
    "        pair_list = pair_list+list(combinations(quad_dum,2))\n",
    "    orbital_list = neut_orbitals_list if len(neut_orbitals_list) > len(prot_orbitals_list) else prot_orbitals_list\n",
    "    for index_for_list in range(matching_lvl,len(orbital_list)):\n",
    "        i = orbital_list[index_for_list]\n",
    "        if len(pair)==2:\n",
    "            pair_list.append(tuple(pair))\n",
    "            pair = []; pair.append(i)\n",
    "        elif len(pair) < 2:\n",
    "            pair.append(i)\n",
    "    if len(pair)==2:\n",
    "        pair_list.append(tuple(pair))\n",
    "        pair = []; pair.append(i)\n",
    "    pair_list.sort()\n",
    "    return pair_list\n",
    "\n",
    "def antipara_spin_pair_gen(obs_onebody_df:\"pandas.DataFrames\", num_nucleon_orbitals:\"tuple\")->\"list\":\n",
    "    # Function that generate list of pair combinations that has spin, S=0 \n",
    "    # obs_onebody_df is the source dataframe for single particle energy levels\n",
    "    # num_nucleon_orbitals is tuple containing the number of neutron and proton\n",
    "    spin_dict = {}\n",
    "    pair_list = []\n",
    "    for index, row in obs_onebody_df.iterrows():\n",
    "        spin_dict[int(row['q_i'])] = int(row[\"spin\"])\n",
    "    neut_index_list = list(range(0,num_nucleon_orbitals[0])); prot_index_list = list(range(num_nucleon_orbitals[0],sum(num_nucleon_orbitals)))\n",
    "    # Upgrade this to a class to show the number of Tz=1 and Tz=0\n",
    "    ## Tz=1\n",
    "    for (neut,prot) in zip(neut_index_list,prot_index_list):\n",
    "        for (neut2,prot2) in zip(neut_index_list,prot_index_list):\n",
    "            S_prot = spin_dict[prot] + spin_dict[prot2]\n",
    "            S_neut = spin_dict[neut] + spin_dict[neut2]\n",
    "            if S_prot == 0:\n",
    "                pair_list.append((prot,prot2))\n",
    "            if S_neut == 0:\n",
    "                pair_list.append((neut,neut2))\n",
    "    ## Tz=0 only\n",
    "    for neut in neut_index_list:\n",
    "        for prot in prot_index_list:\n",
    "            S = spin_dict[neut] + spin_dict[prot]\n",
    "            if S == 0 :\n",
    "                pair_list.append((neut,prot))   ## the opposite (prot,neut) is ignored (not in the real file)\n",
    "    pair_list.sort() ## rearranged to make it easy to read\n",
    "    return pair_list\n",
    "\n",
    "def HFground_pair_list(num_particles:\"tuple\",num_nucleon_orbitals:\"tuple\")->\"list\":\n",
    "    neut_state_init = list(range(0,num_particles[0]))\n",
    "    prot_state_init = list(range(num_nucleon_orbitals[0],num_nucleon_orbitals[0]+num_particles[1]))\n",
    "    nucl_state_init = neut_state_init + prot_state_init\n",
    "    pair_list = list(combinations(nucl_state_init,2))\n",
    "    return pair_list\n",
    "\n",
    "def sin_bod_if_list_gen(num_nucleon_orbitals_:\"tuple\")->\"list\":\n",
    "    # a function that generates (init,fina) list, where init and fina are indeces of single level epsilon\n",
    "    num_neut = num_nucleon_orbitals_[0] ; num_prot = num_nucleon_orbitals_[1]\n",
    "    if_neut = list(combinations(range(0,num_neut),2))\n",
    "    if_prot = list(combinations(range(num_neut,num_neut+num_prot),2))\n",
    "    if_list = if_neut + if_prot\n",
    "    return if_list\n",
    "\n",
    "def extract_number(file:'string')->\"int\":\n",
    "    phrase_interest = re.findall(\"\\D\\D\\D_\\d+\",file)[0] # to ensure the phrase match XXX_NNN whr XXX is string for pcname, and NNN is the code of calculation\n",
    "    s = re.findall(\"\\d+\",phrase_interest)[0]\n",
    "    return (int(s) if s else -1,file)\n",
    "\n",
    "def pathfilename_gen(pcname_:\"string\", input_txt_:\"string\")->\"dict\":\n",
    "    ## Setting up the path (now is directory where compute.py is ran)\n",
    "    current_path = os.getcwd()                     # Current path(which is the Compute dir)\n",
    "    try:\n",
    "        abs_main                                   # Check if the abs_main was defined before\n",
    "    except:\n",
    "        abs_main = os.path.dirname(current_path)   # Main directory containing the compute dir\n",
    "    os.chdir(abs_main)                             # Change dir to main dir(so called absolute main directory)\n",
    "\n",
    "    abspath_data_dir = os.path.join(abs_main,\"Data\")\n",
    "    abspath_result_dir = os.path.join(abs_main,\"Result\")\n",
    "    \n",
    "    #### Extract index for next dir and filename\n",
    "    rel_path_result = os.path.relpath(\"Result\",abs_main)\n",
    "    print((rel_path_result))\n",
    "    rel_PATH_result = (Path(rel_path_result))\n",
    "    subresult_dir_list = [str(x) for x in rel_PATH_result.iterdir() if x.is_dir()]\n",
    "    latest_result_dir = (max(subresult_dir_list,key=extract_number))\n",
    "    new_index = extract_number(latest_result_dir)[0]+1\n",
    "    \n",
    "    #### Create a new directory to keep new results\n",
    "    ##### Extract neucleus name\n",
    "    input_txt_dum = re.split('-|_|\\\\.', input_txt)\n",
    "    nucleus_name = input_txt_dum[1]\n",
    "    output_id = pcname_ + \"_\" + \"{:03d}\".format(new_index) + \"_\"\n",
    "    subresult_dir = os.path.join(rel_path_result,output_id+nucleus_name)\n",
    "    os.mkdir(subresult_dir)\n",
    "\n",
    "    ### Using a dictionary to store the file names to be used\n",
    "    pathfilename = {}\n",
    "    #### Input filenames or path\n",
    "    pathfilename[\"source_text\"] = os.path.join(abspath_data_dir, input_txt)\n",
    "    #### Output # only ready to be input as fermionic op is considered not source\n",
    "    pathfilename[\"output_1B-source_csv\"] =  subresult_dir + \"-1B-source.csv\"\n",
    "    pathfilename[\"output_2B-source_csv\"] =  subresult_dir + \"-2B-source.csv\"\n",
    "    pathfilename[\"output_1B-H_input_csv\"] = subresult_dir + \"-1B-H_input.csv\"\n",
    "    pathfilename[\"output_2B-H_input_csv\"] = subresult_dir + \"-2B-H_input.csv\"\n",
    "    pathfilename[\"config_output_py\"] = subresult_dir + \"_num_config.py\"\n",
    "    pathfilename[\"abstract_result\"] = subresult_dir + \"vqe_abst\"\n",
    "    pathfilename[\"full_result\"] = subresult_dir + \"vqe_full\"\n",
    "    return abs_main, pathfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652300c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06b6694b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df50d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_main, pathfilename = pathfilename_gen(\"Hpc\",input_txt)\n",
    "os.chdir(abs_main)\n",
    "\n",
    "## Reading the txt file and convert to a list object\n",
    "text_whole = []\n",
    "with open (pathfilename[\"source_text\"], 'rt') as myfile:    \n",
    "    for line in myfile:\n",
    "        line = line.rstrip('\\n')\n",
    "        text_whole.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda0272e",
   "metadata": {},
   "source": [
    "# Parameters extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ada263",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters extractions\n",
    "## Extraction of num_particles data; Create a new class of object called num_orbitals\n",
    "## Below the number of particles and orbitals all refer to the valence\"s\n",
    "## Valence!! VALENCE!!\n",
    "pattern_num_particles = re.compile(\"isospin label Omega pi   energy  occupation partner_n partner_p\")\n",
    "num_particles = [None,None]\n",
    "num_nucleon_orbitals = [None,None]\n",
    "\n",
    "## Extract particles number information\n",
    "pattern_energy_start = \"-------------------- Lower bound for Pairing Valence Space -------------------------\"\n",
    "pattern_energy_end   = \"-------------------- Upper bound for Pairing Valence Space -------------------------\"\n",
    "energy_block_id = identify_line(text_whole, pattern_energy_start,pattern_energy_end);\n",
    "energy_block_id.sort()\n",
    "\n",
    "energy_neut_list = text_whole[energy_block_id[0]+1: energy_block_id[1]]\n",
    "energy_prot_list = text_whole[energy_block_id[2]+1: energy_block_id[3]]\n",
    "energy_neut_list = remove_line(energy_neut_list,\"Fermi\")\n",
    "energy_prot_list = remove_line(energy_prot_list,\"Fermi\")\n",
    "\n",
    "num_neut_list = []; num_prot_list = []\n",
    "for line in energy_neut_list:\n",
    "    num_neut_list.append(int(line[53:55]))\n",
    "for line in energy_prot_list:\n",
    "    num_prot_list.append(int(line[53:54]))\n",
    "    \n",
    "### Summary of this Block\n",
    "num_particles[0] = sum(num_neut_list); num_particles[1] = sum(num_prot_list); num_particles = tuple(num_particles)\n",
    "num_neut_orbitals = len(energy_neut_list); num_prot_orbitals = len(energy_prot_list)\n",
    "num_nucleon_orbitals[0] = num_neut_orbitals; num_nucleon_orbitals[1] = num_prot_orbitals; num_nucleon_orbitals = tuple(num_nucleon_orbitals)\n",
    "num_spin_orbitals = sum(num_nucleon_orbitals)\n",
    "### SUMMARY END ###\n",
    "\n",
    "### Getting the hf_lvl index for the lower bound hf states\n",
    "n_start = int(energy_neut_list[0][7:10].strip())\n",
    "p_start = int(energy_prot_list[0][7:10].strip())\n",
    "\n",
    "## config particles and orbitals number\n",
    "with open(os.path.join(abs_main,pathfilename['config_output_py']),'w') as myfile:\n",
    "    myfile.write(\"num_particles = \"+ str(num_particles))\n",
    "    myfile.write(\"\\nnum_nucleon_orbitals = \"+ str(num_nucleon_orbitals))\n",
    "    myfile.write(\"\\nnum_spin_orbitals = \"+ str(num_spin_orbitals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f62a7c",
   "metadata": {},
   "source": [
    "# Twobody Matrix Terms source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f6d526f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Extract twobody matrix elements information\n",
    "extracted_twobody = []\n",
    "pattern_hf = re.compile(\"HF INDEX AND ISOSPIN\")\n",
    "for line in text_whole:\n",
    "    if pattern_hf.search(line) != None:      # If a match is found\n",
    "        extracted_twobody.append((line.rstrip('\\n')).lstrip('HF INDEX AND ISOSPIN'))\n",
    "### Store the information extracted into a dataframe; and output them as csv for later viewing/assessment\n",
    "obs_twobody_df = pd.DataFrame(columns = [\"q_i1\",\"q_i2\",\"q_f1\",\"q_f2\",\"V_ffii\",\"hf_lvl_i1\",\"iso_i1\",\"hf_lvl_i2\",\"iso_i2\",\"hf_lvl_f1\",\"iso_f1\",\"hf_lvl_f2\",\"iso_f2\"])\n",
    "for counter, extracted_twobody_row in enumerate(extracted_twobody):\n",
    "    row_split = extracted_twobody_row.split()\n",
    "    dummy_q_i1 = np.nan; dummy_q_i2 = np.nan; dummy_q_f2 = np.nan; dummy_q_f2 = np.nan; #q is qubit; i1,i2,f1,and f2  qubit indeces\n",
    "    dummy_V_ffii = float(row_split[8])\n",
    "    dummy_hfl_i1 = int(row_split[0]); dummy_iso_i1 = int(row_split[1])\n",
    "    dummy_hfl_i2 = int(row_split[2]); dummy_iso_i2 = int(row_split[3])\n",
    "    dummy_hfl_f1 = int(row_split[4]); dummy_iso_f1 = int(row_split[5])\n",
    "    dummy_hfl_f2 = int(row_split[6]); dummy_iso_f2 = int(row_split[7])\n",
    "\n",
    "    appending_row = [\n",
    "        dummy_q_i1,dummy_q_i2,dummy_q_f2,dummy_q_f2,dummy_V_ffii,\n",
    "        dummy_hfl_i1,dummy_iso_i1,\n",
    "        dummy_hfl_i2,dummy_iso_i2,\n",
    "        dummy_hfl_f1,dummy_iso_f1,\n",
    "        dummy_hfl_f2,dummy_iso_f2]\n",
    "    obs_twobody_df.loc[counter] = appending_row\n",
    "obs_twobody_source_df_output = obs_twobody_df.copy()\n",
    "\n",
    "### Define the qubit indeces\n",
    "#### Start #### create to make the code cleaner, will be looped in the defining values for qubit indeces\n",
    "suffix_list = [\"i1\",\"i2\",\"f1\",\"f2\"]; iso_list=[]; q_list=[]; hf_lvl_list=[]\n",
    "for suffix in suffix_list:\n",
    "    iso_list.append(\"iso_\"+suffix); q_list.append(\"q_\"+suffix); hf_lvl_list.append(\"hf_lvl_\"+suffix)\n",
    "#### End ####\n",
    "for index, row in obs_twobody_source_df_output.iterrows():\n",
    "    for (iso_dum,q_dum,hf_lvl_dum) in zip(iso_list, q_list, hf_lvl_list):\n",
    "        if row[iso_dum] == 1:         ## iso = 1 represents neutron; 2 represents proton\n",
    "            row[q_dum] = row[hf_lvl_dum] - n_start\n",
    "        elif row[iso_dum] == 2:\n",
    "            row[q_dum] = row[hf_lvl_dum] - p_start + num_neut_orbitals\n",
    "\n",
    "### To csv\n",
    "obs_twobody_source_df_output.to_csv(pathfilename[\"output_2B-source_csv\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e6e56",
   "metadata": {},
   "source": [
    "# Onebody Matrix Terms source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b282ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract onebody information\n",
    "## Store the onebody terms into panda dataframes and export as csv\n",
    "obs_onebody_df = pd.DataFrame(columns = [\"q_i\",\"spin\",\"epsilon\",\"hf_lvl\",\"iso\"])\n",
    "\n",
    "for counter, line in enumerate(energy_neut_list+energy_prot_list):\n",
    "    dummy_spin = None\n",
    "    dummy_iso = None\n",
    "    dummy_df_row = []\n",
    "    line = line.replace(')',' ')\n",
    "    line = remove_character(line,'()')\n",
    "    split_list = line.split()\n",
    "    if int(split_list[6]) == 1:\n",
    "        dummy_iso = 1\n",
    "    elif int(split_list[6]) == -1:\n",
    "        dummy_iso = 2\n",
    "    else: \n",
    "        pass\n",
    "\n",
    "    if  float(Fraction(split_list[2])) < 0:\n",
    "        dummy_spin = -1\n",
    "    elif float(Fraction(split_list[2])) > 0:\n",
    "        dummy_spin = 1\n",
    "    else:\n",
    "        pass\n",
    "    dummy_hf_lvl = int(split_list[1])\n",
    "    dummy_epsilon = float(split_list[4])\n",
    "    dummy_df_row = [np.nan,dummy_spin,dummy_epsilon,dummy_hf_lvl,dummy_iso]\n",
    "    obs_onebody_df.loc[counter] = dummy_df_row\n",
    "obs_onebody_source_df_output = obs_onebody_df.copy() ## clone the df so that the below codes doesnt mess things up(action is debatable, but i dont care)\n",
    "\n",
    "### Define the qubit indeces for obs_onebody_df\n",
    "for index, row in obs_onebody_source_df_output.iterrows():\n",
    "    if row[\"iso\"] == 1:\n",
    "        row[\"q_i\"] = row[\"hf_lvl\"] - n_start\n",
    "    elif row[\"iso\"] == 2:\n",
    "        row[\"q_i\"] = row[\"hf_lvl\"] - p_start + num_nucleon_orbitals[0]\n",
    "\n",
    "### To csv\n",
    "obs_onebody_source_df_output.to_csv(pathfilename['output_1B-source_csv'])\n",
    "obs_onebody_df = obs_onebody_source_df_output # Change the df back to the changed values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2fc4b0",
   "metadata": {},
   "source": [
    "# Filter out unwanted interactions from obs_twobody_df\n",
    "# Create obs_twobody_H_input_df_output (input for VQE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe8a0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### obs_twobody_df will always be the name of being_worked_df of two\n",
    "obs_twobody_df = obs_twobody_source_df_output\n",
    "\n",
    "from itertools import combinations\n",
    "## Create interaction of Interest aka excitation_list\n",
    "### Allowed pair combinations\n",
    "# non_repeat_list = list(combinations(range(0,num_spin_orbitals),2))    ## List pair combination that doesn't repeat same orbitals (particle doesnt occupy same orbital twice)\n",
    "degenerate_levels_list = degenerate_pair_gen(num_nucleon_orbitals)      ## Degenerate level list, list of pair combinations that share the same energy\n",
    "spin_pair_list = antipara_spin_pair_gen(obs_onebody_df,num_nucleon_orbitals)  ## list of Pairs with T=1 (include Tz=0,+-1)\n",
    "init_pair_list = HFground_pair_list(num_particles, num_nucleon_orbitals)      ## Allowed Initial state\n",
    "## to allow for set that include blocked levels for Be9, and set that cater for promotions in Be10\n",
    "\n",
    "allowed_init = list(set(degenerate_levels_list) & set(spin_pair_list) & set(init_pair_list))\n",
    "allowed_fina = list(set(degenerate_levels_list) & set(spin_pair_list))\n",
    "\n",
    "\n",
    "### Allowed promotions\n",
    "#### Neutron orbitals (indeces assigned to qubit)\n",
    "neut_orbitals_list = list(range(0,num_nucleon_orbitals[0]))\n",
    "#### Proton orbitals (indeces assigned to qubit)\n",
    "prot_orbitals_list = list(range(num_nucleon_orbitals[0], sum(num_nucleon_orbitals)))\n",
    "excitations_list = []\n",
    "for init in allowed_init:\n",
    "    for fina in allowed_fina:\n",
    "        if ((init[0] in neut_orbitals_list) == (fina[0] in neut_orbitals_list) and\n",
    "            (init[1] in neut_orbitals_list) == (fina[1] in neut_orbitals_list) and\n",
    "            (init[0] in prot_orbitals_list) == (fina[0] in prot_orbitals_list) and\n",
    "            (init[1] in prot_orbitals_list) == (fina[1] in prot_orbitals_list)):\n",
    "            excitations = [init,fina]; excitations.sort();\n",
    "            excitations_list.append(tuple(excitations) if (excitations not in excitations_list) else tuple())\n",
    "excitations_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f544d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a drop_list that containes row index to drop\n",
    "drop_list = []\n",
    "for line_number, (index, row) in enumerate(obs_twobody_df.iterrows()):\n",
    "    init_1 = int(row['q_i1']); init_2 = int(row['q_i2']);\n",
    "    fina_1 = int(row['q_f1']); fina_2 = int(row['q_f2']);\n",
    "    current_promotion = ((init_1,init_2),(fina_1,fina_2))\n",
    "    if (current_promotion in excitations_list):\n",
    "        pass\n",
    "    else:\n",
    "        drop_list.append(line_number)\n",
    "obs_twobody_df.drop(drop_list,axis=0,inplace=True)\n",
    "obs_twobody_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "### To csv\n",
    "obs_twobody_H_input_df_output = obs_twobody_df.copy()\n",
    "obs_twobody_H_input_df_output.to_csv(pathfilename['output_2B-H_input_csv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87a5be7",
   "metadata": {},
   "source": [
    "# Create obs_onebody_H_input_df_output (input for VQE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c76db3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### obs_onebody_df will always be the name of being_worked_df of onebody\n",
    "del obs_onebody_df\n",
    "obs_onebody_df = pd.DataFrame(columns =['q_i','q_f','delta'])\n",
    "\n",
    "### init_fina_list\n",
    "if_list = sin_bod_if_list_gen(num_nucleon_orbitals)\n",
    "epsilon_list = obs_onebody_source_df_output.loc[:,\"epsilon\"]\n",
    "\n",
    "for counter, if_ in enumerate(if_list):\n",
    "    delta = epsilon_list[if_[1]] - epsilon_list[if_[0]]\n",
    "    obs_onebody_df.loc[counter] = [if_[0], if_[1], round(delta,6)]\n",
    "\n",
    "obs_onebody_H_input_df_output = obs_onebody_df.copy()\n",
    "obs_onebody_H_input_df_output.to_csv(pathfilename['output_1B-H_input_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5e4c5c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abcraw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17964/2424635115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mabcraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'abcraw' is not defined"
     ]
    }
   ],
   "source": [
    "abcraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c35d811",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_onebody_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_twobody_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c2aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
